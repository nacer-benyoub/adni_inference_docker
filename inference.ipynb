{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T14:41:20.910003Z","iopub.status.busy":"2024-05-07T14:41:20.909063Z","iopub.status.idle":"2024-05-07T14:41:20.916496Z","shell.execute_reply":"2024-05-07T14:41:20.915410Z","shell.execute_reply.started":"2024-05-07T14:41:20.909967Z"},"id":"L7vSfcNoOeeF","trusted":true},"outputs":[],"source":["import os\n","\n","from get_config import get_config_dict\n","\n","from pathlib import Path\n","\n","import tifffile as tiff\n","\n","import numpy as np\n","import pandas as pd\n","\n","import json\n","import requests\n","\n","config = get_config_dict()"]},{"cell_type":"markdown","metadata":{"id":"lCdootrLYKlo"},"source":["### Load the data"]},{"cell_type":"markdown","metadata":{},"source":["`load_*_data` functions expect ADNI-like directory structure"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T12:22:43.818048Z","iopub.status.busy":"2024-05-07T12:22:43.817544Z","iopub.status.idle":"2024-05-07T12:22:43.826470Z","shell.execute_reply":"2024-05-07T12:22:43.825490Z","shell.execute_reply.started":"2024-05-07T12:22:43.818017Z"},"id":"MnD1RiMFOeeH","trusted":true},"outputs":[],"source":["# Load images\n","def load_2d_data(data_dir: Path):\n","    X = []\n","    ids = {}\n","    for subject_idx, subject_path in enumerate(data_dir.glob('*')):\n","        scan_ids = {}\n","        subject_scans = []\n","        subject_id = subject_path.stem\n","        \n","        for scan_idx, scan_path in enumerate(subject_path.glob('**/I*')):\n","            scan_imgs = []\n","            scan_id = scan_path.stem\n","            scan_ids[scan_id] = []\n","            for img_idx, img_path in enumerate(scan_path.glob('**/*.tiff')):\n","                img_name = img_path.stem\n","                \n","                img = tiff.imread(img_path)\n","                if img.shape[-1] not in (1, 3):\n","                    img = np.expand_dims(img, axis=-1)\n","                if img.shape[-1] == 1:\n","                    img = np.repeat(img, 3, axis=-1)\n","                \n","                scan_imgs.append(img)\n","                scan_ids[scan_id].append(img_name)\n","\n","            subject_scans.append(scan_imgs)\n","            \n","        ids[subject_id] = scan_ids\n","        subject_scans = np.array(subject_scans)\n","        X.append(subject_scans)\n","    return X, ids\n","\n","def load_3d_data(data_dir: Path, npz_key='image'):\n","    X = []\n","    ids = {}\n","    for subject_idx, subject_path in enumerate(data_dir.glob('*')):\n","        scan_ids = {}\n","        subject_scans = []\n","        subject_id = subject_path.stem\n","        \n","        scan_paths = subject_path.glob('**/*.npz')\n","        for scan_idx, scan_path in enumerate(scan_paths):\n","            scan_imgs = []\n","            scan_id = scan_path.parent.stem\n","            scan_name = scan_path.stem\n","            \n","            img = np.load(scan_path)[npz_key]\n","            \n","            scan_imgs.append(img)\n","            scan_ids[scan_id] = scan_name\n","\n","            subject_scans.append(scan_imgs)\n","            \n","        ids[subject_id] = scan_ids\n","        subject_scans = np.array(subject_scans)\n","        X.append(subject_scans)\n","    # X = tf.squeeze(X)\n","    return X, ids\n"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-07T12:22:46.757011Z","iopub.status.busy":"2024-05-07T12:22:46.756365Z","iopub.status.idle":"2024-05-07T12:24:02.093125Z","shell.execute_reply":"2024-05-07T12:24:02.092111Z","shell.execute_reply.started":"2024-05-07T12:22:46.756978Z"},"id":"hDKRUQnsOeeH","outputId":"f22429f5-3305-43d4-dfbf-9e3e7ae4b5c2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of 002_S_0413 images: (7, 30, 180, 180, 3)\n","Shape of 005_S_0221 images: (1, 30, 180, 180, 3)\n"]}],"source":["preprocessed_data_path = config[\"preprocessed_data_path\"]\n","\n","# Define the directory containing the extracted dataset\n","data_dir = Path(preprocessed_data_path)\n","if config['save_2d']:\n","    X, ids = load_2d_data(data_dir)\n","else:\n","    X, ids = load_3d_data(data_dir)\n","    X = [np.squeeze(imgs, axis=1) for imgs in X]\n","    \n","for (idx, subject_imgs), subject_id in zip(enumerate(X), ids.keys()):\n","    # print(subject_imgs.shape)\n","    print(f\"Shape of {subject_id} images:\", subject_imgs.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of 002_S_0413 preds: (7, 30, 3)\n","Shape of 005_S_0221 preds: (1, 30, 3)\n"]}],"source":["serving_url = config['serving_url']\n","\n","# Inference loop\n","y_pred = []\n","for subject_imgs in X:\n","    subject_preds = []\n","    \n","    for scan_imgs in subject_imgs:\n","        # Make prediction request\n","        data = json.dumps({\n","            \"signature_name\": \"serving_default\", # Might parametrize this (include in get_config)\n","            \"inputs\": scan_imgs.tolist()\n","        })\n","        headers = {\"content-type\": \"application/json\"}\n","        json_response = requests.post(serving_url, data=data, headers=headers)\n","        response_dict = json.loads(json_response.text)['outputs']\n","        scan_preds = response_dict['predictions']\n","        # Reverse one-hot predictions\n","        # scan_preds = scan_preds.argmax(axis=-1)\n","        \n","        subject_preds.append(scan_preds)\n","    \n","    y_pred.append(np.array(subject_preds))\n","    \n","for subject_preds, subject_id in zip(y_pred, ids.keys()):\n","    print(f\"Shape of {subject_id} preds:\", subject_preds.shape)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>slice_name</th>\n","      <th>MCI</th>\n","      <th>AD</th>\n","      <th>CN</th>\n","    </tr>\n","    <tr>\n","      <th>subject_id</th>\n","      <th>scan_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">002_S_0413</th>\n","      <th>I120917</th>\n","      <td>ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>1.606320e-04</td>\n","      <td>5.174440e-06</td>\n","      <td>9.998343e-01</td>\n","    </tr>\n","    <tr>\n","      <th>I120917</th>\n","      <td>ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>7.267045e-05</td>\n","      <td>3.921404e-06</td>\n","      <td>9.999233e-01</td>\n","    </tr>\n","    <tr>\n","      <th>I120917</th>\n","      <td>ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>3.601292e-05</td>\n","      <td>8.749840e-07</td>\n","      <td>9.999631e-01</td>\n","    </tr>\n","    <tr>\n","      <th>I120917</th>\n","      <td>ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>2.501315e-05</td>\n","      <td>1.232631e-07</td>\n","      <td>9.999748e-01</td>\n","    </tr>\n","    <tr>\n","      <th>I120917</th>\n","      <td>ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>2.526427e-05</td>\n","      <td>1.728038e-08</td>\n","      <td>9.999747e-01</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">005_S_0221</th>\n","      <th>I102054</th>\n","      <td>ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>4.448588e-05</td>\n","      <td>9.999545e-01</td>\n","      <td>1.124220e-06</td>\n","    </tr>\n","    <tr>\n","      <th>I102054</th>\n","      <td>ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>8.300892e-07</td>\n","      <td>9.999992e-01</td>\n","      <td>3.696108e-08</td>\n","    </tr>\n","    <tr>\n","      <th>I102054</th>\n","      <td>ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>1.867396e-06</td>\n","      <td>9.999980e-01</td>\n","      <td>6.207378e-08</td>\n","    </tr>\n","    <tr>\n","      <th>I102054</th>\n","      <td>ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>1.719506e-07</td>\n","      <td>9.999996e-01</td>\n","      <td>2.876896e-07</td>\n","    </tr>\n","    <tr>\n","      <th>I102054</th>\n","      <td>ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...</td>\n","      <td>4.779064e-08</td>\n","      <td>9.999940e-01</td>\n","      <td>5.971553e-06</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>240 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["                                                           slice_name  \\\n","subject_id scan_id                                                      \n","002_S_0413 I120917  ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...   \n","           I120917  ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...   \n","           I120917  ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...   \n","           I120917  ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...   \n","           I120917  ADNI_002_S_0413_MR_MPR__GradWarp__B1_Correctio...   \n","...                                                               ...   \n","005_S_0221 I102054  ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...   \n","           I102054  ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...   \n","           I102054  ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...   \n","           I102054  ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...   \n","           I102054  ADNI_005_S_0221_MR_MPR__GradWarp__B1_Correctio...   \n","\n","                             MCI            AD            CN  \n","subject_id scan_id                                            \n","002_S_0413 I120917  1.606320e-04  5.174440e-06  9.998343e-01  \n","           I120917  7.267045e-05  3.921404e-06  9.999233e-01  \n","           I120917  3.601292e-05  8.749840e-07  9.999631e-01  \n","           I120917  2.501315e-05  1.232631e-07  9.999748e-01  \n","           I120917  2.526427e-05  1.728038e-08  9.999747e-01  \n","...                          ...           ...           ...  \n","005_S_0221 I102054  4.448588e-05  9.999545e-01  1.124220e-06  \n","           I102054  8.300892e-07  9.999992e-01  3.696108e-08  \n","           I102054  1.867396e-06  9.999980e-01  6.207378e-08  \n","           I102054  1.719506e-07  9.999996e-01  2.876896e-07  \n","           I102054  4.779064e-08  9.999940e-01  5.971553e-06  \n","\n","[240 rows x 4 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# import yaml\n","# with open('CLASSES.yaml') as f:\n","#     label_mapper = yaml.safe_load(f.read())\n","\n","def build_df(y_pred, ids, is_2d):\n","    df = []\n","    for subject_id, subject_preds in zip(ids.keys(), y_pred):\n","        for scan_id, scan_preds in zip(ids[subject_id].keys(), subject_preds):\n","            if is_2d:\n","                for slice_name, slice_pred in zip(ids[subject_id][scan_id], scan_preds):\n","                    d = {\n","                        \"subject_id\": subject_id,\n","                        \"scan_id\": scan_id,\n","                        \"slice_name\": slice_name,\n","                    }\n","                    d.update({\n","                        label: pred for label, pred in zip(response_dict['labels'], slice_pred)\n","                    })\n","                    df.append(d)\n","            else:\n","                for slice_num, slice_pred in enumerate(scan_preds):\n","                    d = {\n","                        \"subject_id\": subject_id,\n","                        \"scan_id\": scan_id,\n","                        \"slice_name\": f'{ids[subject_id][scan_id]}_slice{slice_num}'\n","                    }\n","                    d.update({\n","                        label: pred for label, pred in zip(response_dict['labels'], slice_pred)\n","                    })\n","                    df.append(d)\n","\n","    df = pd.DataFrame(df).set_index(['subject_id', 'scan_id'])\n","    return df\n","\n","if config['save_2d']:\n","    df = build_df(y_pred, ids, is_2d=True)\n","else:\n","    df = build_df(y_pred, ids, is_2d=False)\n","df"]},{"cell_type":"markdown","metadata":{},"source":["### Saving the predictions"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["pred_path = config['pred_path']\n","if not pred_path.exists():\n","    pred_path.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Save slice-level predictions to `slice_predictions.json`\n","slice_pred_path = pred_path.joinpath('slice_predictions.json')\n","\n","df.set_index('slice_name', append=True)\\\n","  .groupby('subject_id')\\\n","  .apply(lambda x:\n","      x\\\n","      # drop subject_id index level\n","      .droplevel('subject_id')\\\n","      # group by scan_id index level\n","      .groupby('scan_id')\\\n","          .apply(lambda y:\n","              y\\\n","              # drop scan_id index level\n","              .droplevel('scan_id')\\\n","              .to_dict(orient='index')\n","          )\n","      .to_dict()\n","  )\\\n","  .to_json(slice_pred_path, orient='index', indent=4)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Save scan-level predictions to `scan_predictions.json`\n","scan_pred_path = pred_path.joinpath('scan_predictions.json')\n","\n","scan_level_preds = df[['MCI', 'AD', 'CN']].groupby(['subject_id', 'scan_id']).aggregate('mean')\n","scan_level_preds.groupby(level=0)\\\n","    .apply(lambda x:\n","        x \\\n","        .droplevel(0)\\\n","        .to_dict(orient='index')\n","    ) \\\n","    .to_json(scan_pred_path, orient='index', indent=4)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Save subject-level predicitons to `subject_predictions.json`\n","subject_pred_path = pred_path.joinpath('subject_predictions.json')\n","\n","scan_level_preds.groupby('subject_id')\\\n","    .mean() \\\n","    .to_json(subject_pred_path, orient='index', indent=4)"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4865537,"sourceId":8285067,"sourceType":"datasetVersion"},{"modelInstanceId":37599,"sourceId":44764,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":37640,"sourceId":44816,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
